---
title: "VRAM 8GBの限界に挑む：20人のAIエージェントがひしめくボクセル世界"
date: 2026-02-06
tags: [Local-LLM, Ollama, Three.js, Performance-Test]
category: 開発日記
---

# VRAM 8GBの限界に挑む：20人のAIエージェントがひしめくボクセル世界

「ローカルLLMで、一体何人までエージェントを同時に動かせるのか？」

今回の実験テーマはシンプル。私の愛機（RTX 3060 Ti / 8GB VRAM）をどこまで追い込めるか、その限界に挑戦しました。結果は、驚くべきことに **「20人体制」** でも安定した稼働を見せました。

## 1. 実験の舞台：Voxel AI Lab
ボクセルで構成された 30x30 の浮島。ここに、自律的に考え、歩き、他者と対話する AI エージェントを投入しました。頭脳には **Llama 3 (8B)** を採用。Ollama を通じて、毎ステップごとにエージェント一人一人が「次の行動」を JSON で生成します。

> [!IMPORTANT] System Specs
> - GPU: NVIDIA GeForce RTX 3060 Ti (8GB VRAM)
> - Model: Llama 3 (8B) - 4-bit Quantized
> - Logic: FastAPI + Three.js

## 2. 衝突から生まれる「社会性」
ただ歩くだけではありません。エージェント同士が 1.0 ボクセル以内に接近すると **「衝突（Collision）」** が発生します。この時、AI の知覚情報（Perception）には `collision_detected: true` という信号が送られます。

ルールは一つ：**「ぶつかったら、相手に挨拶せよ」**。

10人、20人と密度が上がるにつれ、あちこちで「Hello!」「Oops, sorry!」といった吹き出しが飛び交う様子は、まさにデジタルな生態系のようでした。

## 3. パフォーマンス・ベンチマーク
最も懸念していたのは、人数の増加に伴う推論速度の低下（VRAM 溢れによるスワップ）です。しかし、計測データは驚くべき結果を示しました。

| エージェント数 | 1人あたりの推論時間 | 1ターンの周期 | 安定性 |
| :--- | :--- | :--- | :--- |
| 5人 | 約 4.6秒 | 約 23秒 | 完璧 |
| 10人 | 約 4.7秒 | 約 47秒 | 安定 |
| **20人** | **約 4.8秒** | **約 96秒** | **驚異の安定** |

推論を並列（同時）ではなく逐次（順番）に行っているため、一度にロードされるモデルは一つで済み、人数を増やしても 1 人あたりの思考時間はほとんど変化しませんでした。8GB VRAM でも、1分40秒待てるなら 20 人の AI 社会を維持できるのです。

## 4. 結論：ローカルLLMの可能性
今回の実験で、コンシューマ向け GPU であっても、十分に複雑なマルチエージェント・シミュレーションが可能であることが実証されました。

「待ち時間」という課題はありますが、それは非同期処理や軽量モデルの導入で解決可能な範囲です。次は、この 20 人の AI たちに「家を建てさせる」テストでもしてみようかと思います。
